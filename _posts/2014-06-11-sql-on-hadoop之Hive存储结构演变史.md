---
layout: post
title:  "sql on hadoop之hive存储结构的发展史以及性能优化的方向"
date:   2014-06-11
categories: 
- Notes 
tags:
- 分布式系统与计算
---
hive是facebook与2009年开源的一个建立在hadoop之上的大数据分析仓库，并提供类似sql的接口hql.用户输入的sql进过hive的语法语义分析之后依次生成抽象语法书---逻辑查询计划--物理查询计划--最后转变为多个相互依赖的mapreduce job提交给jobtracker进行处理。从
这里可以看出**hive是建立在mr框架之上的，所以hive的定位就在于对海量的离线数据进行分析，而且是适合OLAP而非OLTP**，不用于时下正火的google dremel的开源实现impala&&drill 以及建立在内存计算框架spark之上的shark（这两个东西我会在下一篇文章中说一下）。
hive和它们做对比是不合理的。
自从hive开源至今已经发布到了0.13版本，出现很大的变化。主要体现在多个方面：
1.我所熟悉的存储结构的变化，从textfile到sequencefile到rcfile再到orcfile以及我们自己开发的存储结构fosf。
    
2.sql解析器的优化，比如说map side join 以及start join优化等；
    
3.底层计算引擎的变化，tez计算框架的出现，尚且不稳定；
    
4.解析方式的变化，从一行一行解析到并行多行的解析，也不稳定。
    
5.acid这种事务机制的low level支持。
    
6.前面所说的暂且认为是性能的纵向提高，那么hcatlog这种方便管理元数据的姑且认为是功能的横向扩展吧。
    
**下面我先重点说一下存储结构的演化**
    
textfile和sequencfile都属于按行存储的格式，区别在于后者的存储更加紧凑，节省了一定的存储空间。按行存储的结构主要有两个缺点。前面说过Hive定位于OLAP，那么行存储结构对于一个普通select　a,b from tablec 这样的查询而言会加载太多的查询无关的列到内存里，比如说一个table有100列，我们只需要查询其中少数列，但是加载的时候，我们是需要把整行的数据全部加载到内存里之后才组合出查询相关的列，由此可以看出，行存储加载了太多查询无关的数据到内存里。
    
这是缺点之一，缺点之二是相对于列存储的每一个列的数据类型相同而言，行存储很难有高效的压缩效果。
    
    
于是，便有了列存储的RCFile的出现。其实严格上老说RCFile是行存储和列存储相结合的混合存储方式，对于一张表，我们首先是水平划分对个segment，这样做的目的是为了保证每一行元祖的多个属性列存放在同一个block，这样在查询时元祖重构的时候没有跨节点的网络开销。在水平划分的基础上，对于每一个segment，Rcfile则是按列来存储，起性能的提升正是前面所说的行存储的两个缺点。
    
然而，优化并没有停止。。我们在查询的时候虽然做到了只把查询相关的列加载进入到内存里，对于select　a,b from tablec where c>100  而言，rcfile就是把abc三列全部加载到内存之后再做c>100这个判断条件的筛选。看到这里你可能就会想，为何不只把c列先加载到内存里进行判断，得出满足条件的行号，然后再加载ab列相应的数据进来不就行了么。。对的，但是hive以前的执行逻辑据我所知是是吧abc全部加载到内存之后再判断filter的。所以这里就需要实现filter pushdown机制（这个在后面的hive已经实现了）。
    
好了，实现了这个机制之后，我们只需要加载c列的全部数据到内存里，然和再加载需要的ab列到内存里。。。。。那能不能再优化一点呢。其实可以考虑为每一个的基本粒度page建立一个索引，粗糙的记录这个page的最大值和最小值，这样的话，针对c>100这样的过滤条件，我们只需要先拿这个page的最小值和最大值进行比较（比较结构有postive，negative以及rough三种）。其中rough是性能最差的情况。比如说这个page的最大值是150，最小值是50,也就是说这个page有一部分数据满足filter一部分不满足，称之为rough。那么我们就只能把这个page全部数据加载进来再逐行暴力比价。这也就是我为何称之为粗糙索引的原因，实际情况下要避免rough的出现（后面我会讨论这个）。
   
好了，上述思想的实现者其实有三个，一个是orc，一个是我们team的存储结构的fosf，还有一个就是twitter的parquet，其实这三个存储结构开始的时间点差不多，但结果还是不一样的，orc已经成为了hive最新的高效的存储结构，而我们team的fosf则实在是没落，后来parquet也成为了hive官方支持的存储结构。。这原因嘛是多方面的，确实很懊悔。
   
然而，事情并没有结束，前面我已经说过了，列存储还有一个利好就是压缩效果很好。下面我们就说说压缩算法，上述三种存储结构都用了自己的压缩算法，综合来说实验室自己的fosf的压缩策略要更胜一筹。个人认为，对压缩算法的评价主要有几点：压缩比，压缩时间，解压时间（注意么，这里说的都是无损压缩）。对于hive这种以查询分析为主而言，压缩时间其实是最次考虑到，主要考虑压缩比和解压时间，这需要结合公司自己的需求。一个最适的压缩算法需要考虑到数据的类型和分布情况。我们以int为列，其分布情况主要有重复值很多且连续，重复值很多不连续，相邻数值的增量差比较小，distinct的值很少以及呈现等差数列等情况，每一个类型都有一种已知的最适算法。而对于string而言，事实上，string类型数据是占用存储空间最大的类型。当前我所知道的string类型的压缩主要偶RedBlackTree以及trie树，一般情况下tire效果比较好，字符串的压缩算法这一块个人感觉还有很大的提升空间。
  
**好了，前面说了那么多原理上的东西，事实上上述数据结构在实现过程中还有很多细节性问题**
 
1.粗糙索引的基本page大小以及成本分析。前面说过要尽量避免暴力扫描的出现，直观上来看减小page的粒度，可以使得page的rough情况减少，这是没有问题的。但是，page粒度越小，你的其他开销就增大了。主要包括频繁建立page Index以及查询时频繁计算Index的成本。所以page粒度的临界点确实是个值得研究的问题。
  
事实上，依据我目前所做的benchmark来看，三种存储结构的效果都不理想，只有一种情况是很理想的，你猜到了吗？---------排序之后的index.....
 
2.还有一点就是最适应编码的问题。一种选择是让用户来选择给每一个列的压缩算法，当然，前提是你的用户知道每一个列的分布情况。。
 还有一种则是在加载的过程中动态选择最适压缩了。综合加载成本而言，个人偏好于一第一个page为基准选择，就是在加载第一个page的时候统计出这个数据的列的分布情况，并为其采用多种决策树不能选择的压缩算法来衡量。当这个page的压缩算法确定了之后，我们就为该列剩余的数据选择同样的压缩算法了。。问题又来了，你有想到合适的统计数据分布情况的算法和数据结构了吗？这里可不能用matlab额。。。。
 
3.在这里我想谈谈牺牲精度的近似计算问题。。。。。当数据量很大的时候做一个top k这样的算法其实是很耗费时间的，于是便出现了牺牲精度的近似计算，核心思想是利用了boolmfilter，相关的paper比较多，可以去看看。
 
