---
layout: post
title:  "sql on hadoop之hive存储结构的发展史以及性能优化的方向"
date:   2014-06-11
categories: 
- Notes 
tags:
- 分布式系统与计算
---
hive是facebook于2009年开源的一个建立在hadoop之上的大数据分析仓库，并提供类似sql的接口hql.用户输入的sql经过hive的语法语义分析之后依次生成抽象语法书---逻辑查询计划--物理查询计划--查询优化--最后转变为多个相互依赖的mapreduce job（stages)提交给jobtracker进行处理。从这里可以看出**hive是建立在mr框架之上的，所以hive的定位就在于对海量的离线数据进行分析，而且是适合OLAP而非OLTP，不同于时下正火的google dremel的开源实现impala&&drill 以及建立在内存计算框架spark之上的shark**（这两个东西我会在下一篇文章中说一下）。拿hive和它们做对比是不合理的。


自从hive开源至今已经发布到了0.13版本，出现了很大的变化，横向扩展和纵向扩展。主要体现在多个方面：


**1**.我所熟悉的存储结构的变化，从textfile到sequencefile到rcfile再到orcfile&&parquet以及我们实验室自己开发的存储结构fosf。
    
**2**.sql解析器的优化，比如说map side join 以及star join优化等；

**3**.底层计算引擎的变化，tez计算框架的出现，尚且不稳定 https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez；
    
**4**.解析方式的变化，从一行一行解析到并行多行的解析，也不稳定 https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution

**5** cost based optimized ,很关键，但是据我所知其中的很多优化是建立在TEZ引擎盖之上的，这和其是由主推TEZ引擎的公司主导开发有关吧
    
**6**.acid这种事务机制的low level支持 https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions
    
**7**.前面所说的暂且认为是性能的纵向提高，那么hcatlog这种方便管理元数据的姑且认为是功能的横向扩展吧
    
**下面我重点说一下存储结构的演化**
    
textfile和sequencfile都属于按行存储的格式，区别在于后者的存储更加紧凑，节省了一定的存储空间。按行存储的结构主要有**两个缺点**。 前面说过Hive定位于OLAP，那么行存储结构对于一个普通select　a,b from tablec 这样的查询而言会加载太多的查询无关的列到内存里，比如说一个table有100列，而我们只需要查询其中少数列，但是加载的时候，我们是需要把整行的数据全部加载到内存里之后才组合出查询相关的列，由此可以看出，**行存储加载了太多查询无关的数据列到内存里**。
    
这是缺点之一，缺点之二是**相对于列存储的每一个列的数据类型相同而言，行存储很难有高效的压缩效果**。
    
    
于是，慢慢便有了列存储的RCFile的出现。其实严格上老说**RCFile是行存储和列存储相结合的混合存**方式，对于一张表，我们首先是水平划分多个片断，我称之为segment，这样做的目的是为了保证每一行元组的多个属性列存放在同一个block，这样在查询时元组重构的时候没有跨节点的网络开销，这一点很重要。在水平划分的基础上，对于每一个segment，Rcfile则是按列来存储，其性能的提升正是前面所说的行存储的两个缺点。
    
**然而，优化并没有停止**。。。。。。。我们在查询的时候虽然做到了只把查询相关的列加载进入到内存里，对于select　a,b from table  where c>100  而言，rcfile就是把abc三列全部加载到内存之后再做c>100这个判断条件的筛选。看到这里你可能就会想，为何不只把c列先加载到内存里进行判断，得出满足条件的行号，然后再加载ab列相应的满足过滤条件的数据进来不就行了么？。。对的，但是hive以前的执行逻辑据我所知是是把abc全部加载到内存之后再判断filter的。所以这里就需要实现filter pushdown机制（这个在后面的hive已经实现了）。
    
好了，实现了这个机制之后，我们只需要先加载c列的全部数据到内存里，然和再加载需要的满足条件的ab列到内存里。。。。。那能不能再优化一点呢？  其实可以考虑为每一个的基本粒度page建立一个索引，粗糙的记录这个page的最大值和最小值，这样的话，针对c>100这样的过滤条件，我们只需要先拿这个page的最小值和最大值进行比较（比较结果有postive，negative以及rough三种）。其中rough是性能最差的情况，比如说这个page的最大值是150，最小值是50,也就是说这个page有一部分数据满足filter c>100 一部分不满足，我称之为rough。那么我们就只能把这个page全部数据加载进来再逐行暴力比较。这也就是我为何称之为**粗糙索引**的原因，实际情况下要避免rough的出现（后面我会讨论这个）。
   
好了，上述思想的实现者其实有**三个，一个是orc，一个是我们实验室的存储结构的fosf，还有一个就是twitter的parquet**，其实这三个存储结构开始的时间点差不多，但结果还是不一样的，orc已经成为了hive最新的高效的存储结构，而我们team的fosf则实在是没落（原因很多），后来parquet也成为了hive官方支持的存储结构。。这原因嘛是多方面的，确实很懊悔。
   
**然而，事情并没有结束**，前面我已经说过了，列存储还有一个利好就是压缩效果很好。下面我们就说说压缩算法，上述三种存储结构都用了自己的压缩算法，综合来说实验室自己的fosf的压缩策略胜一筹。个人认为，对压缩算法的评价主要有几点：**压缩比，压缩时间，解压时间（注意，这里说的都是无损压缩）以及压缩算法的通用性**。对于hive这种以查询分析为主的仓库而言，压缩时间一般来说是最次考虑到，主要考虑压缩比和解压时间以及通用性，这需要结合公司自己的需求。**一个最适的压缩算法需要考虑到数据的类型和分布情况**。

我们以int为列，其分布情况主要有**重复值很多且连续，重复值很多不连续，相邻数值的增量差比较小，distinct的值很少以及呈现等差数列等情况**，每一个类型都有一种已知的最适算法。而对于string而言，**事实上，string类型数据是占用存储空间最大的类型**。当前我所知道的string类型的压缩主要偶RedBlackTree以及trie树，一般情况下tire效果比较好，但是比起整形数据的压缩效果而言还是逊色不少**字符串的压缩算法这一块个人感觉还有很大的提升空间**。在针对每一个列采用了最适压缩之后，我们需要在segment外边采用通用压缩，比如说lzo,zlibby以及snappy，这几种压缩和解压缩在数十M/s左右波动。
  
**好了，前面说了那么多原理上的东西，事实上，上述数据结构在实现过程中还有很多细节性问题**
 
1.粗糙索引的基本page大小以及成本分析。前面说过要尽量避免暴力扫描的出现，直观上来看减小page的粒度，可以使得page的rough情况减少，这是没有问题的。 但是，page粒度越小，你的其他开销就增大了。主要包括频繁建立pageIndex以及查询时频繁计算Index的成本。所以page粒度的临界点确实是个值得研究的问题。
  
事实上，依据我目前所做的benchmark来看，三种存储结构的实际filter效果都不理想，只有一种情况是很理想的，你猜到了吗？---------**排序之后的index**.....
 
2.还有一点就是最适应编码的问题。一种选择是让用户来选择给每一个列的压缩算法，当然，前提是你的用户知道每一个列的分布情况。。
 还有一种则是在加载的过程中动态选择最适压缩了,这一点的实现有多重决策。一种是为每一个page在加载的时候同时进行多重压缩，然后采用压缩性能最好的那种，但是这样成本实在是太高。。。综合加载成本而言，个人偏好于参照第一个page为基准选择，就是在加载第一个page的时候统计出这个数据的列的分布情况，并为其采用多种压缩算法来衡量（**其实这里还有一个问题，如果有两种算法的压缩比很相近，那么该如何选择？这里需要设定一个算法选择的决策树了**）。当这个page的压缩算法确定了之后，我们就为该列剩余的数据选择同样的压缩算法了。。**问题又来了，你有想到合适的统计数据分布情况的算法和数据结构了吗？这里可不能用matlab额。。。。**
 
3.在最后我想谈谈牺牲精度的近似计算问题。。。。。当数据量很大的时候做一个top k这样的算法其实是很耗费时间的，如果用户对数据结果的一点误差可以接受，那么可以考虑牺牲精度的近似计算，核心思想是利用了boolmfilter，相关的paper比较多，可以去看看
http://wangmeng.us/notes/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E4%B
8%AD%E7%89%BA%E7%89%B2%E7%B2%BE%E5%BA%A6%E7%9A%84%E8%BF%91%E4%BC%B
C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/


**其实可以看出hive存储结构的演化还是很清楚的，期待以后更加高效的设计结构**
 
